import os, glob, yaml
import pandas as pd

# samples and meta data
build = config["build"]
sample_file = config["sample_file"]
sample_table = pd.read_csv(sample_file, sep='\t')
sample_table.columns = sample_table.columns.str.upper()
header = list(sample_table.columns.values).upper()

# check sample_table for required headers
error_message = []
if "SAMPLE" not in header:
    error_message.append("ERROR: please ensure that you have a column entitled SAMPLE. This is a required column.")

if "BCL_DIR" not in header:
    error_message.append("ERROR: please ensure that you have a column entitled BCL_DIR. This is a required column. \
        This is the path to the BCL data that needs to be demultiplexed. This directory must contain a file called \
        Samplesheet.csv that contains the demultiplexing information (including sample names and indices). This \
        file is typically generated by the sequencing machine software.")

if "RUN" not in header:
    error_message.append("Warning: If multiple samplesheets are provided, it is HIGHLY recommended to include the run number. \
        Otherwise, bcl2fastq QC reports may get overwritten. If RUN is not specified, all outputs will be written to \n\n" +
        config["fastq_dir"] + "/FASTQ_RUN\n\nand the bcl2fastq report outputs (stats/ and reports/) will be written to\n\n"+
        config["bcl2fastq_dir"] + "/RUN\n\n")

if "INDIVIDUAL" not in header:
    error_message.append("Warning: UDN ID not provided. FASTQs will not be copied to clinical directory.")

if "TISSUE" not in header:
    error_message.append("Warning: RNA-seq tissue not provided. FASTQs will not be copied to clinical directory.")

print("\n".join(error_message))

# create dictionary of samplesheet info
def parse_samplesheet(bcldir):
    samplesheet = bcldir + "/SampleSheet.csv"

    # initialize and defaults
    trail=""
    is_data=False
    adapter=""
    adapter2=""

    with open(samplesheet,'r') as f:
        # if samples are listed more than once in samplesheet, we only want to extract
        # info from the first entry, so we will save samples to the list 'seen' as we go
        seen=[]

        for line in f:
            info = line.strip().split(',')

            # if preceding line is "Reads", save first field as read length
            if trail == "[Reads]":
                read_len = info[0]

            # otherwise, save any adapter data
            elif info[0] == "Adapter":
                adapter = info[1]
            elif info[0] == "AdapterRead2":
                adapter2 = info[1]

            # other otherwise, wait until we find the sample data
            # when we reach the column headers, set is_data to true
            # and get the column indices for the Sample_ID, i7 and i5 
            # index sequences, and prepare to track the row index of each sample
            elif trail == "[Data]":
                is_data = True
                sample_row_idx = 0
                sample_col_idx = info.index("Sample_ID")
                index_col_idx = info.index("index")
                index2_col_idx = info.index("index2")
                trail = info[0]
                continue

            # if the current line is not sample data, update the trail
            # if not is_data:
            #     trail = info[0]
            #     continue

            # if the current line is sample data, update the dictionary
            if is_data:
                # get row number and sample ID
                sample_row_idx += 1
                sample = info[sample_col_idx]
                # if sample has already been seen, skip to next line
                if sample in seen:
                    trail = info[0]
                    continue
                # if sample has not already been seen, add it to the list
                # and save information
                seen.append(sample)
                i7_index = info[index_col_idx]
                i5_index = info[index2_col_idx]
                seqinfo_dict['SAMPLE'].append(sample)
                seqinfo_dict['BCL_DIR'].append(bcldir)
                seqinfo_dict["READ_LEN"].append(read_len)
                # seqinfo_dict["ADAPTER"].append(adapter)
                # seqinfo_dict["ADAPTER2"].append(adapter2)
                seqinfo_dict["BCL_ROW"].append(str(sample_row_idx))  #append(f"{sample_row_idx:02d}")
                seqinfo_dict["INDEX1"].append(i7_index)
                seqinfo_dict["INDEX2"].append(i5_index)

            # update the trail
            trail = info[0]

# variables for BCL2FASTQ
BCL_DIRS = set(sample_table.BCL_DIR.values)

# extract sequencing details from SampleSheet.csv
seqinfo_dict = {'SAMPLE':[],
                'BCL_DIR':[],
                'READ_LEN':[],
                # 'ADAPTER':[],
                # 'ADAPTER2':[],
                'BCL_ROW':[],
                'INDEX1':[],
                'INDEX2':[]
                }

for BCL_DIR in BCL_DIRS:
    parse_samplesheet(BCL_DIR)

# update the sample table
seqinfo_table = pd.DataFrame.from_dict(seqinfo_dict)
##this sample table will be it's own config variable, unique from the previous one
sample_table = pd.merge(sample_table, seqinfo_table).set_index("SAMPLE", drop=False)

# save the sample table
save_file = os.path.splitext(sample_file)[0] + '_updated' + os.path.splitext(sample_file)[1]
sample_table.to_csv(save_file, sep='\t')

# get wildcards
sample_table.index.name = None
SAMPLES = list(sample_table.index.values)
RUNS = list(sample_table.RUN.values) if "RUN" in header else []
BCL_ROWS = list(sample_table.BCL_ROW.values)
BCL_BASE_DIRS = [os.path.basename(x) for x in BCL_DIRS]

# get parameters from sample table
def get_readlen(wildcards):
    return sample_table.loc[(wildcards.sample), "READ_LEN"]
    # return sample_table.loc[(wildcards.sample), "READ_LENGTH"] 

def get_index1(wildcards):
    return sample_table.loc[(wildcards.sample), "INDEX1"] 

def get_index2(wildcards):
    return sample_table.loc[(wildcards.sample), "INDEX2"] 

def get_merged_fastq(wildcards, read):
    # this is used by rule run_fastqc to avoid needing {run} in the output
    run = sample_table.loc[(wildcards.sample), "RUN"] if "RUN" in header else ""
    return config["fastq_dir"]+"/FASTQ_RUN"+str(run)+"/{sample}_merge_R2.fastq.gz"

# copy and rename fastqs for GCs
def copy_rename(wildcards, read):
    return config["euan_dir"]+"/"+build+"/FASTQ/"+ \
           sample_table.loc[wildcards.sample]['INDIVIDUAL']+"_"+wildcards.sample+"_"+ \
           sample_table.loc[wildcards.sample]['TISSUE']+ \
           "_merge_R"+str(read)+".fastq.gz"


# make directories if necessary
for dir in [ config["fastq_dir"], config["star_dir"], config["bam_dir"], config["rsem_dir"] ]:
    try:
        os.makedirs(dir, exist_ok = True)
    except OSError as e:
        raise

READS=["1","2"]
rule all:
    input:
        # stats = expand(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/stats", zip, seq_dir = BCL_BASE_DIRS, run=RUNS),
        # reports = expand(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/reports", zip, seq_dir = BCL_BASE_DIRS, run=RUNS),
        R1 = expand(expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{{readnum}}.fastq.gz", 
                           zip, run=RUNS, sample=SAMPLES), readnum=READS),
        # R2 = expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.fastq.gz", zip, run=RUNS, sample=SAMPLES),
        R1_trim = expand(expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{{readnum}}.trimmed.fastq", 
                         zip, run=RUNS, sample=SAMPLES), readnum=READS),
        # R2_trim = expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.trimmed.fastq", zip, run=RUNS, sample=SAMPLES),
        R1_zip = expand(expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{{readnum}}.trimmed.fastq.gz", 
                        zip, run=RUNS, sample=SAMPLES), readnum=READS),
        # R2_zip = expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.trimmed.fastq.gz", zip, run=RUNS, sample=SAMPLES),
        transfer = expand(expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{{readnum}}.sent.log", 
                                 zip, run=RUNS, sample=SAMPLES), readnum=READS),
        fastqc = expand(config["fastqc_dir"]+"/{sample}_merge_R{readnum}.trimmed_fastqc.html", 
                        sample=SAMPLES, readnum=READS),


rule bcl2fastq:
    input:
        bcl2fastq = config["bcl2fastq"],
        bcldir = ancient(expand(config["bcl_dir"]+"/{seq_dir}", seq_dir=BCL_BASE_DIRS))
    params:
        outdir = config["fastq_dir"]+"/FASTQ_RUN{run}"
        # fastq = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_S{bcl_row}_L00{lane}_R{read}_001.fastq.gz"
    output:
        stats = directory(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/stats"),
        reports = directory(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/reports")
    shell:
        """
            {input.bcl2fastq} \
                --runfolder-dir {input.bcldir} \
                --output-dir {params.outdir} \
                --minimum-trimmed-read-length 50 \
                --stats-dir {output.stats} \
                --reports-dir {output.reports} \
                --loading-threads  15 \
                --processing-threads 15  \
                --writing-threads 15
        """

rule merge_fastq:
    input:
        # stats = expand(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/stats", zip, seq_dir=BCL_BASE_DIRS, run=RUNS),
        # reports = expand(config["bcl2fastqc_dir"]+"/{seq_dir}/FASTQ_RUN{run}/reports", zip, seq_dir=BCL_BASE_DIRS, run=RUNS),
        fastqdir = ancient(config["fastq_dir"]+"/FASTQ_RUN{run}")
    params:
        R1 = expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_S{bcl_row}_L00{lane}_R1_001.fastq.gz",
                    zip, run=RUNS, sample=SAMPLES, bcl_row=BCL_ROWS, lane=["1","2"]),
        R2 = expand(config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_S{bcl_row}_L00{lane}_R2_001.fastq.gz",
                    zip, run=RUNS, sample=SAMPLES, bcl_row=BCL_ROWS, lane=["1","2"])
    output:
        R1 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R1.fastq.gz",
        R2 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.fastq.gz"
    shell:
        """
        zcat {params.R1} | gzip > {output.R1}; wait
        zcat {params.R2} | gzip > {output.R2}; wait
        """

rule trim:
    input: 
        R1 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R1.fastq.gz",
        R2 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.fastq.gz"
        # sample_sheet=get_samplesheet
    params:
        short_reads = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_shortreads.fastq.gz",
        index1=get_index1,
        index2=get_index2,
        read_len=get_readlen,
        this_sample = "{sample}"
    output:
        R1 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R1.trimmed.fastq",
        R2 = config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.trimmed.fastq"
    shell:
        """
            set -e
            module load cutadapt/2.4;
            echo "{params.read_len}"
            my_read_len="{params.read_len}"
            echo $my_read_len
            if [[ $my_read_len -lt 100 ]]
            then
                	min_len=20
            else
                	min_len=50
            fi
            echo $min_len
            cutadapt -a {params.index1} -A {params.index2} -o {output.R1} -p {output.R2} -m $min_len --too-short-paired-output={params.short_reads} {input.R1} {input.R2}
            sleep 1
        """
rule zip:
    input:
        R1=config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R1.trimmed.fastq",
        R2=config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.trimmed.fastq"
    params:
        this_sample="{sample}"
    output:
        R1=config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R1.trimmed.fastq.gz",
        R2=config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R2.trimmed.fastq.gz"
    run:
        shell("""
            gzip -fc {input.R1} > {output.R1}
            gzip -fc {input.R2} > {output.R2}
        """)

rule run_fastqc:
    input:
        fastqc=config["fastqc"],
        # file=config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{readnum}.trimmed.fastq.gz"
        file=lambda wildcards: get_merged_fastq(wildcards.sample, read = wildcards.readnum)
    params:
        outdir=config["fastqc_dir"]
    output:
        outreport=config["fastqc_dir"]+"/{sample}_merge_R{readnum}.trimmed_fastqc.html"
    shell:
        """
        mkdir -p {params.outdir}
        {input.fastqc} -o {params.outdir} {input.file}
        """

rule copy_fastq:
    input:
        config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{readnum}.fastq.gz"
    params:
        lambda wildcards: copy_rename(wildcards.sample, read = wildcards.readnum)
    output:
        config["fastq_dir"]+"/FASTQ_RUN{run}/{sample}_merge_R{readnum}.sent.log"
    shell:
        """
        rsync -a {input} {params} &&
        date=$(date) &&
        echo "$date\ncp {input} {params}" > {output}
        """